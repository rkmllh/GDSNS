{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD3Klj6XJd5K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import ast\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = re.sub(r\"\\bcan't\\b\", \"can not\", text)\n",
        "    text = re.sub(r\"\\bn't\\b\", \" not\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_lexical_features(tokens: list) -> dict:\n",
        "    total = len(tokens)\n",
        "    uniq = len(set(t.text for t in tokens))\n",
        "    lengths = [len(t.text) for t in tokens]\n",
        "    freq = Counter(t.text for t in tokens)\n",
        "    hapax = sum(1 for _, c in freq.items() if c == 1)\n",
        "\n",
        "    pos_counts = Counter(tok.pos_ for tok in tokens)\n",
        "    pos_dist = {p: round(pos_counts[p] / total, 3) for p in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]}\n",
        "\n",
        "    bigrams = [(tokens[i].pos_, tokens[i+1].pos_) for i in range(total - 1)]\n",
        "    big6 = Counter(bigrams).most_common(5)\n",
        "\n",
        "    return {\n",
        "        \"total_tokens\": total,\n",
        "        \"unique_tokens\": uniq,\n",
        "        \"type_token_ratio\": round(uniq / total, 3) if total > 0 else 0,\n",
        "        \"avg_token_length\": round(sum(lengths) / total, 3) if total > 0 else 0,\n",
        "        \"hapax_legomena_ratio\": round(hapax / total, 3) if total > 0 else 0,\n",
        "        **{f\"pct_{p.lower()}\": pos_dist.get(p, 0) for p in pos_dist},\n",
        "        \"top_pos_bigrams\": big6\n",
        "    }\n",
        "\n",
        "df_full = pd.read_csv(\"Training_Essay_Data.csv\")\n",
        "df_sample, _ = train_test_split(\n",
        "    df_full,\n",
        "    train_size=700,\n",
        "    stratify=df_full[\"generated\"],\n",
        "    random_state=42\n",
        ")\n",
        "df = df_sample.reset_index(drop=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "#  Parsing, token-level, stats e NER\n",
        "# --------------------------------------------------\n",
        "records_tokens = []\n",
        "records_stats = []\n",
        "records_entities = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    doc_id = int(idx)\n",
        "    raw = row[\"text\"]\n",
        "    label = row[\"generated\"]\n",
        "\n",
        "    # Normalizza e parsifica\n",
        "    norm = normalize_text(raw)\n",
        "    doc = nlp(norm)\n",
        "\n",
        "    # --- token-level & syntactic features ---\n",
        "    token_objs = [tok for tok in doc if not tok.is_space]\n",
        "    for tok in token_objs:\n",
        "        records_tokens.append({\n",
        "            \"doc_id\":      doc_id,\n",
        "            \"label\":       label,\n",
        "            \"sentence\":    tok.sent.start,\n",
        "            \"position\":    tok.i - tok.sent.start,\n",
        "            \"token\":       tok.text,\n",
        "            \"lemma\":       tok.lemma_,\n",
        "            \"pos\":         tok.pos_,\n",
        "            \"tag\":         tok.tag_,\n",
        "            \"is_stop\":     tok.is_stop,\n",
        "            \"dep\":         tok.dep_,\n",
        "            \"head\":        tok.head.text,\n",
        "            \"head_pos\":    tok.head.pos_,\n",
        "            \"is_root\":     tok.dep_ == \"ROOT\",\n",
        "            \"depth\":       len(list(tok.ancestors)),\n",
        "            \"num_children\":len(list(tok.children))\n",
        "        })\n",
        "\n",
        "    feats = extract_lexical_features(token_objs)\n",
        "    records_stats.append({\n",
        "        \"doc_id\":        doc_id,\n",
        "        \"label\":         label,\n",
        "        \"num_sentences\": len(list(doc.sents)),\n",
        "        **feats\n",
        "    })\n",
        "\n",
        "    # --- named entity extraction ---\n",
        "    for ent in doc.ents:\n",
        "        records_entities.append({\n",
        "            \"doc_id\":       doc_id,\n",
        "            \"label\":        label,\n",
        "            \"entity_text\":  ent.text,\n",
        "            \"entity_label\": ent.label_,\n",
        "            \"start_char\":   ent.start_char,\n",
        "            \"end_char\":     ent.end_char,\n",
        "            \"sentence_idx\": ent.sent.start\n",
        "        })\n",
        "\n",
        "\n",
        "df_tokens   = pd.DataFrame(records_tokens)\n",
        "df_stats    = pd.DataFrame(records_stats)\n",
        "df_entities = pd.DataFrame(records_entities)\n",
        "\n",
        "df_tokens.to_csv(\"deep_parsed_tokens.csv\", index=False)\n",
        "df_stats.to_csv(\"lexical_stats.csv\",       index=False)\n",
        "df_entities.to_csv(\"named_entities.csv\",    index=False)\n",
        "\n",
        "print(\"3 file generati: deep_parsed_tokens.csv, lexical_stats.csv, named_entities.csv\")"
      ],
      "metadata": {
        "id": "xTl_Cp19LUT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bigrams(bigram_str):\n",
        "    try:\n",
        "        return ast.literal_eval(bigram_str)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Apply parsing if necessary (only if loaded from CSV)\n",
        "df_stats['top_pos_bigrams'] = df_stats['top_pos_bigrams'].apply(parse_bigrams)\n",
        "\n",
        "records_bigrams = []\n",
        "\n",
        "for _, row in df_stats.iterrows():\n",
        "    doc_id = row['doc_id']\n",
        "    label = row['label']\n",
        "    bigrams = row['top_pos_bigrams']  # list of ((POS1, POS2), count)\n",
        "\n",
        "    for bigram_tuple, count in bigrams:\n",
        "        pos1, pos2 = bigram_tuple\n",
        "        pos_bigram_str = f\"{pos1}_{pos2}\"\n",
        "        records_bigrams.append({\n",
        "            'doc_id': doc_id,\n",
        "            'label': label,\n",
        "            'pos_bigram': pos_bigram_str,\n",
        "            'count': count\n",
        "        })\n",
        "\n",
        "df_bigrams = pd.DataFrame(records_bigrams)\n",
        "\n",
        "df_bigrams.to_csv(\"pos_bigrams.csv\", index=False)"
      ],
      "metadata": {
        "id": "y0shdLh54hcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/lexical_stats.csv\")\n",
        "df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "df.to_csv(\"/content/lexical_stats.csv\")\n",
        "df.to_csv(\"/content/lexical_stats.csv\", index=False)"
      ],
      "metadata": {
        "id": "OqJn-4o--6Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}